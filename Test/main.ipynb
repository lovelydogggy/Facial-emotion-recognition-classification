{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT NAME OF TA TEST FOLDER; make sure folder is located in /data/ repotrain_set\n",
      "../data/train_set/points/\n",
      "Is this path correct? Please type yes or noyes\n",
      "thank you\n",
      "NOTE: SCRIPT WILL NOT RUN IF PATH IS WRONG\n",
      "PREPROCESS DATA STARTING\n",
      "(2000, 3003) (500, 3003)\n",
      "(2000, 3003) (500, 3003)\n",
      "PREPROCESS DATA HAS FINISHED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF\n",
    "import warnings\n",
    "#################DATA PREPROCESSING CLASS###################3\n",
    "\n",
    "from features import FiducialDataProcess, NMF_Data\n",
    "\n",
    "TA_test_folder_name = input('INSERT NAME OF TA TEST FOLDER; make sure folder is located in /data/ repo')\n",
    "\n",
    "'''\n",
    "#This Class is in features.py\n",
    "class FiducialDataProcess(object):\n",
    "\n",
    "\tdef __init__(self, path, num_data, num_features):\n",
    "\t\tself.path = path # \n",
    "\t\tself.num_data = num_data\n",
    "\t\tself.num_features = num_features\n",
    "\t\tself.feature_data = np.zeros((num_data, num_features*(num_features-1)))#np.zeros((2500,78*77))\n",
    "\n",
    "\tdef euc_dist(self, p1, p2):\n",
    "\t\tdist = ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5\n",
    "\t\treturn int(dist)\n",
    "\n",
    "\n",
    "\tdef printkeys(self,i):\n",
    "\t\tdata = loadmat(self.path + '{0:04}'.format(i) +  '.mat')\n",
    "\t\treturn data\n",
    "\n",
    "\tdef preprocess(self):\n",
    "\n",
    "\t\ttotal_features = []\n",
    "\t\tfor i in range(1, self.num_data+1):\n",
    "\t\t\tcurr_feature = []\n",
    "\t\t\tdata = loadmat(self.path + '{0:04}'.format(i) +  '.mat')\n",
    "\n",
    "\t\t\tif 'faceCoordinatesUnwarped' in data:\n",
    "\t\t\t\tarr = data['faceCoordinatesUnwarped']\n",
    "\t\t\telse:\n",
    "\t\t\t\tarr = data['faceCoordinates2']\n",
    "\n",
    "\t\t\tfor j in range(arr.shape[0]-1):\n",
    "\t\t\t\tfor k in range(j+1, arr.shape[0]):\n",
    "\t\t\t\t\tcurr_feature.append(self.euc_dist(arr[j],arr[k]))\n",
    "\t\t\ttotal_features.append(curr_feature)\n",
    "\t\treturn total_features\n",
    "\n",
    "\tdef return_features(self):\n",
    "\t\tt = self.preprocess()\n",
    "\t\treturn t\n",
    "\n",
    "'''\n",
    "\n",
    "####################DATA PREPROCESSING SCRIPT ########################## \n",
    "\n",
    "path = '../data/' + TA_test_folder_name + '/points/' #SPECIFY FOLDER PATH WHERE THE NEW FIDUCIAL DATAPOINTS ARE\n",
    "print(path)\n",
    "check = input('Is this path correct? Please type yes or no')\n",
    "\n",
    "if check == 'yes':\n",
    "    print('thank you')\n",
    "elif check == 'no':\n",
    "    print('example path should be ../data/TEST_SET/points/')\n",
    "    print('Please specify EXACT path where the /points/ folder is located in :')\n",
    "else:\n",
    "    print('Input error; please rerun entire script again')\n",
    "    \n",
    "print('NOTE: SCRIPT WILL NOT RUN IF PATH IS WRONG')    \n",
    "path_, dirs, files = next(os.walk(path))\n",
    "file_count = len(files)\n",
    "\n",
    "num_features = 78\n",
    "num_data = file_count\n",
    "feature_array = FiducialDataProcess(path, num_data, num_features)\n",
    "final_features = np.array(feature_array.return_features())\n",
    "\n",
    "\n",
    "############################# FEATURE ENGINEERING CLASS###########################################\n",
    "import pickle\n",
    "'''\n",
    "#this class is in features.py\n",
    "class NMF_Data(object):\n",
    "\tdef __init__(self, dat_x, dat_y):\n",
    "\t\tself.dat_x = dat_x\n",
    "\t\tself.dat_y = dat_y\n",
    "\t\tself.nmf_features = []\n",
    "\t\tself.nmf = None\n",
    "\n",
    "\tdef create_nmf(self, reduc_comp=100, test_size=500):\n",
    "\t\tx_train, x_test, y_train, y_test = train_test_split(self.dat_x, self.dat_y, random_state=1, test_size = test_size)\n",
    "\t\tprint(x_train.shape, x_test.shape)\n",
    "\t\t\n",
    "\t\tself.nmf = NMF(n_components=reduc_comp, random_state=0)\n",
    "\t\tself.nmf.fit(x_train)\n",
    "\n",
    "\t\tx_train_nmf = self.nmf.transform(x_train)\n",
    "\t\tx_test_nmf = self.nmf.transform(x_test)\n",
    "\n",
    "\t\tself.nmf_features.append(x_train_nmf)\n",
    "\t\tself.nmf_features.append(y_train)\n",
    "\t\tself.nmf_features.append(x_test_nmf)\n",
    "\t\tself.nmf_features.append(y_test)\n",
    "\n",
    "\tdef nmf_dim_reduc(self, data):\n",
    "\t\treturn self.nmf.transform(data)\n",
    "\n",
    "\tdef get_nmf_features(self):\n",
    "\t\treturn self.nmf_features\n",
    "\n",
    "\tdef save_nmf(self, filename):\n",
    "\t\tself.create_nmf()\n",
    "\t\tnp.save(filename, self.nmf_features)\n",
    "'''\n",
    "#################################### FEATURE ENGINEERING SCRIPT ##########################################\n",
    "\n",
    "print('PREPROCESS DATA STARTING')\n",
    "url1 = 'dense_data_type_and_emot.csv'\n",
    "original_data = np.genfromtxt(url1, delimiter=',')\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(original_data)\t\n",
    "features_init = original_data[:,2:]\n",
    "features = features_init / features_init.max(axis=0)\n",
    "labels = original_data[:,0:2] \n",
    "\n",
    "orig_bin_nmf_features = NMF_Data(features, labels)\n",
    "orig_bin_nmf_features.create_nmf(reduc_comp=100)\n",
    "new_bin_nmf_features = orig_bin_nmf_features.nmf_dim_reduc(final_features)\n",
    "x_test_set_bin = new_bin_nmf_features\n",
    "\n",
    "orig_emot_nmf_features = NMF_Data(features, labels)\n",
    "orig_emot_nmf_features.create_nmf(reduc_comp=300)\n",
    "new_emot_nmf_features = orig_emot_nmf_features.nmf_dim_reduc(final_features)\n",
    "x_test_set_emot = new_emot_nmf_features\n",
    "\n",
    "test_set_len = len(x_test_set_bin)\n",
    "\n",
    "print('PREPROCESS DATA HAS FINISHED')\n",
    "\n",
    "#nmf_features  = np.load('nmf_features_type_emot.npy',allow_pickle=True) #for debugging\n",
    "#nmf_features2  = np.load('nmf_features_type_emot_300.npy',allow_pickle=True) # for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING NEW DATA NOW WITH TRAINED NEURAL NETWORKS\n",
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
      "SCRIPT FINISHED; PLEASE LOOK FOR FILE ENTITLED: Proj3_Section1_Group8_TEST_labels.csv\n"
     ]
    }
   ],
   "source": [
    "############################ TESTING EXECUTIVE ############################\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras.models import load_model\n",
    "#warnings.filteringwarnings('ignore')\n",
    "\n",
    "print('TESTING NEW DATA NOW WITH TRAINED NEURAL NETWORKS')\n",
    "#We are loading our pretrained h5 models for our 3 neural networks here\n",
    "#all specific training scripts can be found in training.py\n",
    "\n",
    "url2 = 'binary_classification_model.h5'\n",
    "binary_model = load_model(url2,  custom_objects={'leaky_relu': tf.nn.leaky_relu})\n",
    "binary_predictions = binary_model.predict(x_test_set_bin)\n",
    "binary_predictions = [binary_predictions[i].argmax() for i in range(len(binary_predictions))]   \n",
    "\n",
    "url3= 'h5_models/compound_classification_model.h5'\n",
    "url4= 'h5_models/simple_classification_model.h5'\n",
    "compound_model = load_model(url3,  custom_objects={'leaky_relu': tf.nn.leaky_relu})\n",
    "simple_model = load_model(url4,  custom_objects={'leaky_relu': tf.nn.leaky_relu})\n",
    "\n",
    "emotion_dict = {1: 'Neutral', 2: 'Happy', 3: 'Sad', 4: 'Angry', 5:'Surprised',\n",
    "\t\t\t\t6: 'Disgusted', 7:'Fearful', 8:'Happily surprised', 9: 'Happily disgusted',\n",
    "\t\t\t\t10: 'Sadly angry', 11: 'Angrily disgusted', 12: 'Appalled', 13: 'Hatred',\n",
    "\t\t\t\t14:'Angrily surprised', 15:'Sadly surprised', 16:'Disgustedly surprised',\n",
    "\t\t\t\t17:'Fearfully surprised', 18:'Awed', 19:'Sadly fearful', 20:'Fearfully disgusted',\n",
    "\t\t\t\t21:'Fearfully angry', 22:'Sadly disgusted'}\n",
    "\n",
    "\n",
    "emot_idx = [0]*test_set_len\n",
    "idx = [i for i in range(test_set_len)]\n",
    "\n",
    "compound_prediction = compound_model.predict(x_test_set_emot)\n",
    "simple_prediction = simple_model.predict(x_test_set_emot)\n",
    "\n",
    "for i in range(len(x_test_set_bin)):\n",
    "\tif binary_predictions[i] == 1:\n",
    "\t\temot_idx[i] = compound_prediction[i,:].argmax() + 8\n",
    "\t\tbinary_predictions[i] = 'compound'\n",
    "\n",
    "\telif binary_predictions[i] == 0:\n",
    "\t\temot_idx[i] = simple_prediction[i,:].argmax() + 1\n",
    "\t\tbinary_predictions[i] = 'simple'\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data={\" \": idx, \"Index\": idx, \n",
    "\t\t\t\t\t\t\t\"identity\": [None]*len(x_test_set_bin), \"emotion_idx\": emot_idx,\n",
    "\t\t\t\t\t\t\t\"emotion_cat\": [emotion_dict[emot_idx[i]] for i in range(test_set_len)], \"type\": binary_predictions})\n",
    "\n",
    "df.to_csv(\"./Proj3_Section1_Group8_TEST_labels.csv\", sep=',',index=False)\n",
    "print('SCRIPT FINISHED; PLEASE LOOK FOR FILE ENTITLED: Proj3_Section1_Group8_TEST_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow                         2.0.0            \n",
      "tensorflow-estimator               2.0.1            \n",
      "tensorflow-estimator-2.0-preview   2.0.0            \n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
